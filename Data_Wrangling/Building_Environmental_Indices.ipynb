{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building grids to merge environmental data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "import geopandas as gdp\n",
    "import pandas as pd\n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Point\n",
    "from shapely import speedups\n",
    "speedups.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data (update to where you stored local files)\n",
    "grid = gdp.read_file('D:/Documents/SpringBoard/capstone-1/datasets/gridded-shapefile.shp') \n",
    "    # read in the gridded shapefile\n",
    "\n",
    "# Make sure the grid has the right crs\n",
    "crs = {'init': 'epsg:4326'}\n",
    "grid = gdp.GeoDataFrame(grid, crs=crs)\n",
    "grid = grid.loc[:,['OBJECTID', 'geometry']] # only keep the object ID values and the geometry values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll do this first for SST data\n",
    "sst = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/SST_2012.csv', skiprows=1) \n",
    "    # read in shapefile and skip the first row (because it's just an extra header)\n",
    "    # colnames are: UTC, degrees_north, degrees_east, degree_C\n",
    "    # where degrees_north = latitude and degrees_east = longitude\n",
    "    # Some housekeeping... for SST\n",
    "sst = sst.dropna().reset_index(drop=True) # only keep the sst data that are not NAs\n",
    "sst = sst.drop_duplicates(subset=['degrees_east', 'degrees_north']) # only keep unique lat/lon\n",
    "\n",
    "# Turn the SST data into a geometric dataset\n",
    "geometry = [Point(xy) for xy in zip(sst['degrees_east'], sst['degrees_north'])] \n",
    "sst_geom = gdp.GeoDataFrame(sst, crs=crs, geometry=geometry)\n",
    "\n",
    "sst_polygons = sjoin(sst_geom, grid, how='left') # merge these datasets\n",
    "sst_polygons = sst_polygons.loc[:,['degrees_north', 'degrees_east', 'OBJECTID']] # only keep these three columns\n",
    "sst_polygons['OBJECTID'] = sst_polygons['OBJECTID'].astype(str) # make sure the polygon ID is a string\n",
    "sst_polygons['OBJECTID'] = sst_polygons['OBJECTID'].str.split('.', n=1, expand=True)[0] # and remove the silly .0\n",
    "sst_polygons.columns = ['degrees_north', 'degrees_east', 'Zone'] # rename the columns\n",
    "sst_polygons = sst_polygons[sst_polygons.Zone != 'nan'].reset_index(drop=True) # only keep data\n",
    "    # that have zone IDs\n",
    "sst_polygons.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/sst_locations.csv') # and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's move on to SSS\n",
    "sss = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/SSS_2012.csv', skiprows=1) \n",
    "    # read in shapefile and skip the first row (because it's just an extra header)\n",
    "    # colnames are: UTC, degrees_north, degrees_east, sss-unit\n",
    "    # where degrees_north = latitude and degrees_east = longitude\n",
    "sss = sss.dropna().reset_index(drop=True) # only keep the data that are not NAs\n",
    "sss = sss.drop_duplicates(subset=['degrees_east', 'degrees_north']) # only keep unique lat/lng\n",
    "\n",
    "# Turn the data into a geometric dataset\n",
    "geometry = [Point(xy) for xy in zip(sss['degrees_east'], sss['degrees_north'])]\n",
    "sss_geom = gdp.GeoDataFrame(sss, crs=crs, geometry=geometry)\n",
    "\n",
    "sss_polygons = sjoin(sss_geom, grid, how='left') # merge these datasets\n",
    "sss_polygons = sss_polygons.loc[:,['degrees_north', 'degrees_east', 'OBJECTID']] # only keep these three columns\n",
    "sss_polygons['OBJECTID'] = sss_polygons['OBJECTID'].astype(str) # make sure the polygon ID is a string\n",
    "sss_polygons['OBJECTID'] = sss_polygons['OBJECTID'].str.split('.', n=1, expand=True)[0] # and remove the silly .0\n",
    "sss_polygons.columns = ['degrees_north', 'degrees_east', 'Zone'] # rename the columns\n",
    "sss_polygons = sss_polygons[sss_polygons.Zone != 'nan'].reset_index(drop=True) # only keep data\n",
    "    # that have zone IDs\n",
    "sss_polygons.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/sss_locations.csv') # and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now chl-a\n",
    "chla = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/2012_chl-a.csv', skiprows=1) \n",
    "    # read in shapefile and skip the first row (because it's just an extra header)\n",
    "    # colnames are: UTC, degrees_north, degrees_east, chla-unit\n",
    "    # where degrees_north = latitude and degrees_east = longitude\n",
    "chla = chla.dropna().reset_index(drop=True) # only keep the data that are not NAs\n",
    "chla = chla.drop_duplicates(subset=['degrees_east', 'degrees_north']) # only keep unique lat/lng\n",
    "\n",
    "# Turn the data into a geometric dataset\n",
    "geometry = [Point(xy) for xy in zip(chla['degrees_east'], chla['degrees_north'])] \n",
    "chla_geom = gdp.GeoDataFrame(chla, crs=crs, geometry=geometry) \n",
    "\n",
    "chla_polygons = sjoin(chla_geom, grid, how='left') # merge these datasets\n",
    "chla_polygons = chla_polygons.loc[:,['degrees_north', 'degrees_east', 'OBJECTID']] # only keep these three columns\n",
    "chla_polygons['OBJECTID'] = chla_polygons['OBJECTID'].astype(str) # make sure the polygon ID is a string\n",
    "chla_polygons['OBJECTID'] = chla_polygons['OBJECTID'].str.split('.', n=1, expand=True)[0] # and remove the silly .0\n",
    "chla_polygons.columns = ['degrees_north', 'degrees_east', 'Zone'] # rename the columns\n",
    "chla_polygons = chla_polygons[chla_polygons.Zone != 'nan'].reset_index(drop=True) # only keep data\n",
    "    # that have zone IDs\n",
    "chla_polygons.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/chla_locations.csv') # and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seafloor to gradient\n",
    "depth = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/seafloor_depth_gradient.csv', skiprows=1) \n",
    "    # read in shapefile and skip the first row (because it's just an extra header)\n",
    "    # colnames are: UTC, degrees_north, degrees_east, m\n",
    "    # where degrees_north = latitude and degrees_east = longitude\n",
    "depth = depth.dropna().reset_index(drop=True) # only keep the data that are not NAs\n",
    "depth = depth.drop_duplicates(subset=['degrees_east', 'degrees_north']) # only keep unique lat/lng\n",
    "\n",
    "depth['degrees_east_neg'] = depth['degrees_east']-360 # we have to convert the longitude because it's \n",
    "    # in a different format for this dataset \n",
    "    \n",
    "# Turn the data into a geometric dataset\n",
    "geometry = [Point(xy) for xy in zip(depth['degrees_east_neg'], depth['degrees_north'])] \n",
    "depth_geom = gdp.GeoDataFrame(depth, crs=crs, geometry=geometry)\n",
    "\n",
    "depth_polygons = sjoin(depth_geom, grid, how='left') # merge these datasets\n",
    "depth_polygons = depth_polygons.loc[:,['degrees_north', 'degrees_east', 'OBJECTID']] # only keep these three columns\n",
    "depth_polygons['OBJECTID'] = depth_polygons['OBJECTID'].astype(str) # make sure the polygon ID is a string\n",
    "depth_polygons['OBJECTID'] = depth_polygons['OBJECTID'].str.split('.', n=1, expand=True)[0] # and remove the silly .0\n",
    "depth_polygons.columns = ['degrees_north', 'degrees_east', 'Zone'] # rename the columns\n",
    "depth_polygons = depth_polygons[depth_polygons.Zone != 'nan'].reset_index(drop=True) # only keep data\n",
    "    # that have zone IDs\n",
    "depth_polygons.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/depth_locations.csv') # and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we've got our grids ready to merge with our environmental data. \n",
    "Next: Go to the Merging Environmental Data notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
