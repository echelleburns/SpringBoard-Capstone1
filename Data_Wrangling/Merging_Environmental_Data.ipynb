{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Environmental Data\n",
    "Ok now it's time to merge the grids that we generated with the real datasets... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's start with the sst data... \n",
    "sst_grid = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/sst_locations.csv')\n",
    "    # Load in the datasets\n",
    "sst_grid = sst_grid[['degrees_north', 'degrees_east', 'Zone']] # keep only these columns\n",
    "\n",
    "sst_files = glob.glob('D:/Documents/SpringBoard/capstone-1/datasets/SST*') # get the list of SST datasets\n",
    "for file in sst_files: # for each file\n",
    "    sst_data = pd.read_csv(file, skiprows=1) # read in the data\n",
    "    sst_data = sst_data.merge(sst_grid, how='inner') # and merge\n",
    "    file_name = file.split('\\\\')[1] # split off the file name\n",
    "    sst_data.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/' + file_name, index=False)\n",
    "        # and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now the sss data... \n",
    "sss_grid = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/sss_locations.csv')\n",
    "    # load in the dataset\n",
    "sss_grid = sss_grid[['degrees_north', 'degrees_east', 'Zone']] # only keep these columns\n",
    "\n",
    "sss_files = glob.glob('D:/Documents/SpringBoard/capstone-1/datasets/SSS*') # get the list of SSS datasets\n",
    "for file in sss_files: # for each file\n",
    "    sss_data = pd.read_csv(file, skiprows=1) # read in the data\n",
    "    sss_data = sss_data.merge(sss_grid, how='inner') # and merge\n",
    "    file_name = file.split('\\\\')[1] # split off the file name\n",
    "    sss_data.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/' + file_name, index=False)\n",
    "        # and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now the chla data... \n",
    "chla_grid = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/chla_locations.csv')\n",
    "    # load in the dataset\n",
    "chla_grid = chla_grid[['degrees_north', 'degrees_east', 'Zone']] # only keep these columns\n",
    "\n",
    "chla_files = glob.glob('D:/Documents/SpringBoard/capstone-1/datasets/*chl-a.csv') # get the list of chla datasets\n",
    "for file in chla_files: # for each file\n",
    "    chla_data = pd.read_csv(file, skiprows=1) # read in the data\n",
    "    chla_data = chla_data.merge(chla_grid, how='inner') # and merge\n",
    "    file_name = file.split('\\\\')[1] # split off the file name\n",
    "    chla_data.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/' + file_name, index=False)\n",
    "        # and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 762 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now the benthic slope data... \n",
    "d_grid = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/grid_merges/depth_locations.csv')\n",
    "    # load in the dataset\n",
    "d_grid = d_grid[['degrees_north', 'degrees_east', 'Zone']] # only keep these columns\n",
    "\n",
    "d_data = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/seafloor_depth_gradient.csv', skiprows=1)\n",
    "    # there's only one benthic slope dataset, so read in it\n",
    "d_data = d_data.merge(d_grid, how='inner') # merge \n",
    "d_data.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/seafloor_depth_gradient.csv', index=False)\n",
    "    # and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty painless. Next up: Merging to Master Datasets notebook. Almost at the finish line! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
