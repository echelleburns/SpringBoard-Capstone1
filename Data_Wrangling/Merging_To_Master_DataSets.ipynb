{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Master Dataset\n",
    "So now we can merge them all into several large data files - separated by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant packages\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by merging the shark detection data with the specifics of the shark tagging data\n",
    "shark_detect = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/edited_files/shark_detection_data2.csv')\n",
    "    # read in this dataset\n",
    "shark_detect = shark_detect[['Transmitter', 'Zone', 'Date', 'Lat', 'Lng']] \n",
    "    # only keep these columns\n",
    "shark_detect = shark_detect.astype({'Transmitter':'category', 'Zone':'int64', \n",
    "                                    'Date':'category', 'Lat':'float64', 'Lng':'float64'})\n",
    "    # And make sure the types are appropraite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "shark_meta = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/tagged_sharks.csv')\n",
    "    # read in the shark tagging deployment datasheet\n",
    "shark_meta = shark_meta[['Transmitter', 'animal_weight', 'animal_length_total', 'gender', 'location', 'year']]\n",
    "    # keep these values\n",
    "shark_meta = shark_meta.astype({'Transmitter':'category', 'animal_weight': 'float64', \n",
    "                                'animal_length_total':'float64', 'gender':'category', \n",
    "                                'location':'category', 'year':'category'})\n",
    "    # And make sure the types are appropraite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the shark detection data with the transmitter deployment data and reset the index\n",
    "shark_all = shark_detect.merge(shark_meta, how='inner').reset_index(drop=True)\n",
    "    # we do an inner join because we only want to keep data that are present in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's merge the number of receivers per day\n",
    "rec_day = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/receiver_density_2.csv')\n",
    "    # read in the receiver density files\n",
    "rec_day = rec_day[['Date', 'Receiver_D', 'Zone']] # only keep the relevant columns\n",
    "rec_day['Date'] = rec_day['Date'].str.split(' ', expand=True, n=1)[0] # make sure the date is in the proper format\n",
    "#rec_day['Zone'] = rec_day['Zone'].astype('str') # make sure the zone is a string\n",
    "#rec_day['Zone'] = rec_day['Zone'].str.split('.', expand=True, n=1)[0] # and get rid of the silly .0\n",
    "# rec_day = rec_day.astype('category') # all values can be considered categories at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And merge all the shark detection data with the receiver data and reset the index\n",
    "shark_all = shark_all.merge(rec_day, how='outer').reset_index(drop=True)\n",
    "    # This time we do an outer merge because we want to keep all data, \n",
    "    # even if no sharks were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transmitter</th>\n",
       "      <th>Zone</th>\n",
       "      <th>Date</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "      <th>animal_weight</th>\n",
       "      <th>animal_length_total</th>\n",
       "      <th>gender</th>\n",
       "      <th>location</th>\n",
       "      <th>year</th>\n",
       "      <th>Receiver_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>268175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1291014</td>\n",
       "      <td>2019-10-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1291015</td>\n",
       "      <td>2019-10-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1296043</td>\n",
       "      <td>2019-10-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1298528</td>\n",
       "      <td>2019-10-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1198628</td>\n",
       "      <td>2019-10-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transmitter     Zone        Date  Lat  Lng  animal_weight  \\\n",
       "268175         NaN  1291014  2019-10-10  NaN  NaN            NaN   \n",
       "268176         NaN  1291015  2019-10-10  NaN  NaN            NaN   \n",
       "268177         NaN  1296043  2019-10-10  NaN  NaN            NaN   \n",
       "268178         NaN  1298528  2019-10-10  NaN  NaN            NaN   \n",
       "268179         NaN  1198628  2019-10-10  NaN  NaN            NaN   \n",
       "\n",
       "        animal_length_total gender location year  Receiver_D  \n",
       "268175                  NaN    NaN      NaN  NaN           1  \n",
       "268176                  NaN    NaN      NaN  NaN           0  \n",
       "268177                  NaN    NaN      NaN  NaN           0  \n",
       "268178                  NaN    NaN      NaN  NaN           0  \n",
       "268179                  NaN    NaN      NaN  NaN           0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shark_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok so now we can merge other data... lets do the environmental data by year... \n",
    "dep = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/seafloor_depth_gradient.csv')\n",
    "    # the depth gradient data will not change so we will read it in outside of the loop\n",
    "dep = dep[['Zone', 'm']] # we only really need the zone and the depth value\n",
    "dep.columns = ['Zone', 'DepthGradient'] # and we can rename the columns\n",
    "dep = dep.astype({'Zone':'int64', 'DepthGradient':'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok so now we can merge other data... lets do the environmental data by year... \n",
    "#dep = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/seafloor_depth_gradient.csv')\n",
    "    # the depth gradient data will not change so we will read it in outside of the loop\n",
    "#dep = dep[['Zone', 'm']] # we only really need the zone and the depth value\n",
    "#dep.columns = ['Zone', 'DepthGradient'] # and we can rename the columns\n",
    "#dep = dep.astype({'Zone':'int64', 'DepthGradient':'float64'})\n",
    "\n",
    "years = range(2012, 2020) # this is the range of years that we have, 2012 to 2019\n",
    "#years = range(2012,2013)\n",
    "\n",
    "shark_all['DetYear'] = pd.to_datetime(shark_all.Date).dt.year\n",
    "    # let's change the date value to detection year, so that we can parse these data into years and\n",
    "    # hopefully speed up our data processing.\n",
    "\n",
    "for year in years: # for each year\n",
    "    shark_temp = shark_all[shark_all.DetYear == year] # subset the detection data\n",
    "    env_files = glob.glob('D:/Documents/SpringBoard/capstone-1/datasets/final_files/*'+str(year)+'*')\n",
    "        # and get a list of environmental files that are relevant to that year\n",
    "    for file in env_files: # for each environmental file\n",
    "        temp_env = pd.read_csv(file) # read in the file\n",
    "        temp_env['deg_N_'+str(temp_env.columns[-2])] = temp_env['degrees_north']\n",
    "            # make a latitude column that is unique to that environmental dataset\n",
    "        temp_env['deg_E_'+str(temp_env.columns[-3])] = temp_env['degrees_east']\n",
    "            # make a longitude column that is unique to that environmental dataset\n",
    "        these_cols = temp_env.columns[[0,-1,-2,-3,-4]] # now, we only want the date column, \n",
    "            # the zone information (-1), the environmental information (-2), and the unique\n",
    "            # latitude (-3), and longitude (-4) columns\n",
    "        temp_env = temp_env[these_cols] # let's just keep those columns in the dataset\n",
    "        temp_env['Date'] = temp_env['UTC'].str.split('T', expand=True, n=1)[0] # and we want\n",
    "            # to split the UTC values into just days (they have a weird T present)\n",
    "        temp_env = temp_env.drop(labels='UTC', axis=1) # we want to drop the UTC column and\n",
    "            # just keep the days\n",
    "        temp_env = temp_env.astype('category') # all values can be considered categories\n",
    "        temp_env = temp_env.astype({'Zone': 'int64'})\n",
    "        if file == env_files[0]: # if this is the first file in the list\n",
    "            comb_dat = temp_env.merge(shark_temp, how='outer') # merge it with the shark data itself\n",
    "        else: # but if it's any subsequent file\n",
    "            comb_dat = temp_env.merge(comb_dat, how='outer') # merge it with the already merged list\n",
    "                # these merges will go by zone and date only bc those are the two columns that should\n",
    "                # be the same across each dataframe\n",
    "    comb_dat = dep.merge(comb_dat, how='right').reset_index(drop=True) # merge again with the depth\n",
    "        # data and make sure to reset the index\n",
    "    comb_dat.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/combined/'\n",
    "                    +str(year)+'-without_moon2.csv', index=False)\n",
    "        # and save the data each year\n",
    "        # moon data will come later using pylunar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now, we want to fill in the data gaps where the environmental data were NAs\n",
    "files_fill = glob.glob('D:/Documents/SpringBoard/capstone-1/datasets/final_files/combined/*2.csv')\n",
    "    # first lets get a list of the fiels that we want to iterate through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_fill: # for each of these files\n",
    "    comb_dat = pd.read_csv(file) # read in the file\n",
    "    \n",
    "    # Set up\n",
    "    N_cols = ['deg_N_degree_C', 'deg_N_PSU', 'deg_N_mg m^-3', 'Lat'] \n",
    "        # save the column names for latitude\n",
    "    E_cols = ['deg_E_degree_C', 'deg_E_PSU', 'deg_E_mg m^-3', 'Lng']\n",
    "        # save the column names for longitude\n",
    "        \n",
    "    # Calculate\n",
    "    lats = comb_dat.loc[:, N_cols] # grab the latitudes of all environmental datasets\n",
    "    lats_mean = lats.apply(np.mean, axis=1) # and get the mean of all latitudes for each row\n",
    "    lngs = comb_dat.loc[:, E_cols] # grab all the longitudes of all environmental datasets\n",
    "    lngs_mean = lngs.apply(np.mean, axis=1) # and get the mean of all longitudes for each row\n",
    "    \n",
    "    # Clean up\n",
    "    comb_dat = comb_dat.drop(columns=N_cols) # remove the excess longitude columns\n",
    "    comb_dat = comb_dat.drop(columns=E_cols) # remove the excess latitude columns\n",
    "    \n",
    "    # Update\n",
    "    comb_dat['Lat'] = lats_mean # make the latitude value the mean latitude for each row\n",
    "    comb_dat['Lng'] = lngs_mean # make the longitude value the mean longitude for each row\n",
    "    \n",
    "    # Sort\n",
    "    comb_dat = comb_dat.sort_values(by=['Date','Lat', 'Lng']) # order the data by date, lat, then lon\n",
    "    comb_dat = comb_dat.dropna(subset=['Lat', 'Lng']) # and remove any lat/lngs that are NA's\n",
    "        # it means there's literally no geospatial data associated with these zones, so they're irrelevant\n",
    "    \n",
    "    # Fill NAs\n",
    "    nans = comb_dat[['DepthGradient', 'degree_C', 'PSU', 'mg m^-3']].fillna(method='bfill')\n",
    "        # So we have to do a double fill here, because for most of the data the backfill method will work\n",
    "        # so we do that first for all the environmental data values that are na... \n",
    "        # we save it to a new dataframe\n",
    "    nans = nans[['DepthGradient', 'degree_C', 'PSU', 'mg m^-3']].fillna(method='ffill')\n",
    "        # But the very end of the dataset cannot be backfilled (there's nothing in front of it to fill)\n",
    "        # so we'll do a front fill for the very end\n",
    "        # Now there should be no NA values\n",
    "    comb_dat['Receiver_D'] = comb_dat['Receiver_D'].fillna(0) # we also need to remember to fill \n",
    "        # NAs where there are no acoustic receivers present; there are no data because there are 0\n",
    "        # receivers in the water, so we can just fill NAs with 0\n",
    "    \n",
    "    # Clean up\n",
    "    comb_dat = comb_dat.drop(columns = ['DepthGradient', 'degree_C', 'PSU', 'mg m^-3'])\n",
    "        # let's drop these messy NA filled columns... \n",
    "    comb_dat['DepthGradient'] = nans['DepthGradient'] # and replace them with the non-NA datasets\n",
    "    comb_dat['TempC'] = nans['degree_C'] # We will replace them with names that are more intuitive\n",
    "    comb_dat['Sal'] = nans['PSU'] # salinity\n",
    "    comb_dat['ChlA'] = nans['mg m^-3'] # chlorophyll a\n",
    "    comb_dat = comb_dat.drop(columns=['DetYear']) # we don't need detection year anymore\n",
    "    comb_dat = comb_dat.reset_index(drop=True) # and we want to reset the index\n",
    "    \n",
    "    # Save it\n",
    "    filename = file.split('\\\\')[1] # grab the file name from the list of files\n",
    "    comb_dat.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/combined/filled/'+filename, index=False)\n",
    "        # and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, all that's left is to add the moon phase data... phew.\n",
    "Go to the Adding Moon Phases notebook next. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
