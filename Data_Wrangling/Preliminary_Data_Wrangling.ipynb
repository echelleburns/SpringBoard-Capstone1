{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling - The Beginning\n",
    "This sheet should be run first, in order to get an idea of the geospatial range of acoustic telemetry data that we have, so that we know the spatial extent of environmental data that we need to grab from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Point\n",
    "from shapely import speedups\n",
    "speedups.enable() # makes geospatial merges quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the receiver logs - when and where a receiver was put into the water\n",
    "receiver_log = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/Receiver_Deployment_Log.csv', parse_dates=True)\n",
    "receiver_log = receiver_log[['Station', 'Lat', 'Lng', 'In_PSTPDT', 'Out_PSTPDT', 'Receiver SN']] \n",
    "    # we only care about these columns\n",
    "\n",
    "receiver_log.Lat = pd.to_numeric(receiver_log.Lat, errors='coerce') # make sure the latitude is numeric\n",
    "receiver_log.Lng = pd.to_numeric(receiver_log.Lng, errors='coerce') # make sure the longitude is numeric\n",
    "\n",
    "print(receiver_log[['Lat','Lng']].describe()) # describe the data so that we know where to grab environmental data\n",
    "\n",
    "# ok so we need our data to basically be between 32.9 and 35.4 N and -120.9 and -118.2 W\n",
    "# I will use those limits (sometimes rounded to ints) to download the relevant environmental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to make sure the datetimes are aware and convert them to UTC, because all other\n",
    "# datasets are in UTC time\n",
    "receiver_log['In_PSTPDT'] = pd.to_datetime(receiver_log['In_PSTPDT'], infer_datetime_format=True, errors='coerce')\n",
    "receiver_log['Out_PSTPDT'] = pd.to_datetime(receiver_log['Out_PSTPDT'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "receiver_log['In_UTC'] = receiver_log['In_PSTPDT'].dt.tz_localize('America/Los_Angeles').dt.tz_convert('UTC')\n",
    "receiver_log['Out_UTC'] = receiver_log['Out_PSTPDT'].dt.tz_localize('America/Los_Angeles').dt.tz_convert('UTC')\n",
    "\n",
    "receiver_log = receiver_log.drop(['In_PSTPDT', 'Out_PSTPDT'], axis=1) # no longer care for the PDT/PST times\n",
    "receiver_log = receiver_log.dropna(subset=['In_UTC','Out_UTC']) \n",
    "    # remove NAs (receivers that are still deployed or that were lost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step: See in which zones these data fall..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data (update to where you stored local files)\n",
    "grid = gdp.read_file('D:/Documents/SpringBoard/capstone-1/datasets/trunc/gridded-shapefile.shp') \n",
    "    # read in the gridded shapefile\n",
    "\n",
    "# And only keep the unique receiver locations. \n",
    "just_locations = receiver_log[['Lng','Lat']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the SST data into a geometric dataset\n",
    "geometry = [Point(xy) for xy in zip(just_locations['Lng'], just_locations['Lat'])]\n",
    "\n",
    "# Coordinate reference system : WGS84\n",
    "crs = {'init': 'epsg:4326'}\n",
    "\n",
    "# Create a Geographic data frame \n",
    "just_locations_geom = gpd.GeoDataFrame(just_locations, crs=crs, geometry=geometry)\n",
    "\n",
    "# Make sure the grid is the same crs\n",
    "grid = gpd.GeoDataFrame(grid, crs=crs)\n",
    "grid = grid.loc[:,['OBJECTID', 'geometry']] # only keep the object ID values and the geometry values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_locations_polygons = sjoin(just_locations_geom, grid, how='left') \n",
    "    # join the gps data with the polygon data to get the object ID value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the receiver ID codes so that they are strings and not numbers\n",
    "receiver_log['Receiver SN'] = receiver_log['Receiver SN'].astype(str) # convert to string\n",
    "receiver_log['Receiver SN'] = receiver_log['Receiver SN'].str.split('.', n=1, expand=True)[0] # get rid of the silly .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we're done cleaning our receiver deployment log, we can start calculating\n",
    "# how many receivers are in the water (in each zone) each day. \n",
    "\n",
    "time_sequence = pd.date_range(receiver_log['In_UTC'].dt.date.min(), receiver_log['Out_UTC'].dt.date.max(), tz='UTC')\n",
    "    # We start by building a list of dates that we want data for (basically every day since the first\n",
    "    # receiver went in the water, to the last day a receiver was pulled out of the water)\n",
    "\n",
    "receivers_per_zone_per_day = pd.DataFrame([]) # we want to build an empty dataframe to add to\n",
    "\n",
    "for date in time_sequence: # for each day ...\n",
    "    for zone in receiver_log['Zone'].drop_duplicates(): # for each unique zone\n",
    "        receivers_in = receiver_log[(date > receiver_log['In_UTC']) &  \n",
    "                                    # get a list of the receivers that were deployed before that day\n",
    "                                    (date < receiver_log['Out_UTC']) & \n",
    "                                    # and that were pulled out of the water after that day\n",
    "                                    (receiver_log['Zone'] == zone)]\n",
    "                                    # and that were in the appropriate zone\n",
    "        no_receivers = receivers_in['Station'].drop_duplicates().shape[0]\n",
    "            # find the number of receivers by counting how many station names are in the new series\n",
    "        new_df = pd.DataFrame({'Date': date, 'Receiver_D': no_receivers, 'Zone':zone}) \n",
    "            # make a pd dataframe that saves the date, receiver density, and the zone ID\n",
    "        receivers_per_zone_per_day = receivers_per_zone_per_day.append(new_df, ignore_index=True)\n",
    "            # append these data to the new pd dataframe\n",
    "\n",
    "# And save this to a csv for later...\n",
    "receivers_per_zone_per_day.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/receiver_density.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have our receiver density figured out.\n",
    "## Next, we want to make sure our shark detection data only includes data when we <i>know</i> receivers were in the water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the shark detection data\n",
    "shark_detections = pd.read_csv('D:/Documents/SpringBoard/capstone-1/datasets/AllYOYDetections_filtered.csv', parse_dates=True)\n",
    "shark_detections = shark_detections[['Date_Time', 'Receiver', 'Transmitter', 'Station.Name', 'Latitude', 'Longitude']]\n",
    "    # only keep these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to make these data datetime objects and aware\n",
    "shark_detections['Date_Time'] = pd.to_datetime(shark_detections['Date_Time'], infer_datetime_format=True, errors='coerce')\n",
    "shark_detections['Date_Time'] = shark_detections['Date_Time'].dt.tz_localize('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to make sure that the Receiver column matches the Receiver SN column \n",
    "# in the receiver deployment log dataset, so we need to remove the 'VR2W-' part and make it a category\n",
    "shark_detections['Receiver'] = shark_detections['Receiver'].str.split('-', n=1, expand=True)[1]\n",
    "shark_detections['Receiver'] = shark_detections['Receiver'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to go through the receiver data and only keep the data that are present when a shark lab receiver is in the water. \n",
    "good_detection_data = pd.DataFrame([]) # make an empty dataframe\n",
    "\n",
    "for row in range(len(receiver_log)): # and for each row in receiver log\n",
    "    receiver_id = receiver_log['Receiver SN'][row] # get the receiver id value\n",
    "    det_in_range = shark_detections[(shark_detections.Date_Time > receiver_log['In_UTC'][row]) &\n",
    "                                    # keep shark detection data that were collected after the receiver went in\n",
    "                                    (shark_detections.Date_Time < receiver_log['Out_UTC'][row]) & \n",
    "                                    # and before the receiver came out of the water\n",
    "                                    (shark_detections.Receiver == receiver_id)]\n",
    "                                    # and only use detections from the relevant receiver.\n",
    "    if det_in_range.shape[0] > 0: # if there are sharks present...\n",
    "        det_in_range.loc[:,'Lat'] = receiver_log.loc[row,'Lat'] # fill in the receiver latitude from the log\n",
    "            # (sometimes the detection data do not have updated location data associated with them)\n",
    "        det_in_range.loc[:,'Lng'] = receiver_log.loc[row,'Lng'] # fill in the receiver longitude from the log\n",
    "        det_in_range = det_in_range.drop(['Latitude', 'Longitude'], axis=1)\n",
    "            # drop the old latitude and longitude\n",
    "        good_detection_data = good_detection_data.append(det_in_range, ignore_index=True)\n",
    "            # and add these data to the new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a little clean up\n",
    "good_detection_data = good_detection_data.drop_duplicates() # remove any duplicates that may have occurred\n",
    "good_detection_data = pd.merge(good_detection_data, receiver_log[['Lat', 'Lng', 'Zone']]) \n",
    "    # we want to keep the zone information for these data that we calculated for the receiver log\n",
    "good_detection_data['Date'] = good_detection_data['Date_Time'].dt.date # and we want to make sure \n",
    "    # there is a Date column, because detection data are in YYYY-mm-dd HH:MM:SS and that's too fine-scale\n",
    "    # for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, save these data for later, when we combine it with the environmental datasets.\n",
    "good_detection_data.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/edited_files/shark_detection_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, move onto the Data Wrangling Shapefiles notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
