{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's grab a pseudo-random subset of all the data that equally samples data with sharks and data without sharks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('D:/Documents/SpringBoard/capstone-1/datasets/final_files/combined/filled/*all.csv')\n",
    "    # get a list of all the files that we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jackknifing Method 1\n",
    "\n",
    "This method takes random samples from the nearest 10 zones, but does not check to see whether there are receivers deployed in the zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = [] # make an empty list that we can add to\n",
    "\n",
    "for file in files: # for each file\n",
    "    dat = pd.read_csv(file, # load in the file\n",
    "                  dtype={'Zone':'int64', 'Transmitter':'category', \n",
    "                         'animal_weight':'float64','animal_length_total':'float64', \n",
    "                         'gender':'category', 'location':'category', 'year':'category',\n",
    "                         'Receiver_D':'float64', 'Lat':'float64', 'Lng':'float64', \n",
    "                         'DepthGradient':'float64', 'TempC':'float64', 'Sal':'float64',\n",
    "                         'ChlA':'float64', 'MoonPhase':'category'}, \n",
    "                 parse_dates = ['Date'])\n",
    "    year = dat['Date'].dt.year[0]+1 # figure out which year it is, and add one to it\n",
    "    dat = dat[dat['Date'] < (str(year)+'-01-01')] # so that we can subset the data and make sure we\n",
    "        # only have the data for the year we care about (data were overlapped about 1 day upon download to make)\n",
    "        # sure that all values were collected\n",
    "    \n",
    "    # Housekeeping.. \n",
    "    dat['Transmitter'] = dat['Transmitter'].cat.add_categories(['NoSharks'])\n",
    "        # add a category to replace NAs when no sharks are present\n",
    "    dat['Transmitter'] = dat['Transmitter'].fillna(value='NoSharks') # and replace it with 'NoSharks'\n",
    "    \n",
    "    # Separate the data into data with sharks and data without sharks\n",
    "    shark_dat = dat[dat['Transmitter'] != 'NoSharks'] # with sharks\n",
    "    no_sharks = dat[dat['Transmitter'] == 'NoSharks'] # without sharks\n",
    "    \n",
    "    if len(shark_dat) > 0 : # if there are actually shark data\n",
    "        all_samples.append(shark_dat.drop_duplicates()) # add the shark data to the running\n",
    "            # dataframe\n",
    "    \n",
    "        # And make a range of values from which we can grab data (it's best if we grab \n",
    "        # 'no shark' data from the same/or close zones to which we have shark data for. \n",
    "        # Then we aren't comparing super offshore areas where there are never receivers\n",
    "        # to nearshore areas where there are more receivers)\n",
    "        zone_ids = shark_dat.drop_duplicates(subset='Zone').reset_index(drop=True)\n",
    "            # remove duplicate zones and reset the index\n",
    "        zone_ids = zone_ids['Zone'] # we only really care about the zones\n",
    "        possible_zones = [] # make a new blank list for all the zones that we want to \n",
    "            # grab data from\n",
    "        zone_ids = pd.to_numeric(zone_ids) # make sure the zones are numeric\n",
    "        for row, value in enumerate(zone_ids): # and go through each zone\n",
    "            possible = list(range(value-10, value+11)) # to make a range of possible zones\n",
    "                # that are close to or equal to zones that have shark data\n",
    "            possible_zones.extend(possible) # extend that running list.\n",
    "        possible_zones = pd.DataFrame(possible_zones) # make the possible zones into a\n",
    "            # data frame\n",
    "        possible_zones = possible_zones.drop_duplicates() # and remove duplicates\n",
    "       \n",
    "        # Use these zones to randomly sample the non-shark dataset\n",
    "        to_sample = no_sharks[no_sharks['Zone'].isin(list(possible_zones[0]))]\n",
    "            # make a dataframe that includes zones that are present in possible_zones\n",
    "        left_out = no_sharks[~no_sharks['Zone'].isin(list(possible_zones[0]))]\n",
    "            # and just in case, make one that includes zones that are not present\n",
    "            # in possible zones\n",
    "        if len(to_sample) >= len(shark_dat): # if there are enough NoShark data ...\n",
    "            sample = to_sample.sample(n=len(shark_dat)) # then randomly sample the \n",
    "                # NoShark data so that there's an equal number of shark vs non-shark data\n",
    "            all_samples.append(sample) # and add that to the all_samples\n",
    "        else: # otherwise, if there are not enough shark data\n",
    "            all_samples.append(to_sample) # add all the data in to_sample\n",
    "            backup_sample = left_out.sample(n=(len(shark_dat)-len(to_sample)))\n",
    "                # and take the remaining amount of data from teh backup dataset\n",
    "            all_samples.append(backup_sample) # add the backup data to the all_samples\n",
    "            print(file) # and let me know which files this happens for (in case I decide\n",
    "                # to take those data out later)\n",
    "        \n",
    "all_samples = pd.concat(all_samples) # finally, make this into a big pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/combined/filled/jackknifed.csv', index=False)\n",
    "    # save the file without index values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jackknifing Method 2\n",
    "\n",
    "This method keeps all data that has at least 1 or more receivers deployed, regardless of if there are sharks or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = [] # make an empty list that we can add to\n",
    "\n",
    "for file in files: # for each file\n",
    "    dat = pd.read_csv(file, # load in the file\n",
    "                  dtype={'Zone':'int64', 'Transmitter':'category', \n",
    "                         'animal_weight':'float64','animal_length_total':'float64', \n",
    "                         'gender':'category', 'location':'category', 'year':'category',\n",
    "                         'Receiver_D':'float64', 'Lat':'float64', 'Lng':'float64', \n",
    "                         'DepthGradient':'float64', 'TempC':'float64', 'Sal':'float64',\n",
    "                         'ChlA':'float64', 'MoonPhase':'category'}, \n",
    "                 parse_dates = ['Date'])\n",
    "    year = dat['Date'].dt.year[0]+1 # figure out which year it is, and add one to it\n",
    "    dat = dat[dat['Date'] < (str(year)+'-01-01')] # so that we can subset the data and make sure we\n",
    "        # only have the data for the year we care about (data were overlapped about 1 day upon download to make)\n",
    "        # sure that all values were collected\n",
    "    \n",
    "    # Housekeeping.. \n",
    "    dat['Transmitter'] = dat['Transmitter'].cat.add_categories(['NoSharks'])\n",
    "        # add a category to replace NAs when no sharks are present\n",
    "    dat['Transmitter'] = dat['Transmitter'].fillna(value='NoSharks') # and replace it with 'NoSharks'\n",
    "    \n",
    "    # Separate the data into data with sharks and data without sharks\n",
    "    shark_dat = dat[(dat['Transmitter'] != 'NoSharks') & (dat['Receiver_D'] > 0.0)] # with sharks\n",
    "    no_sharks = dat[(dat['Transmitter'] == 'NoSharks') & (dat['Receiver_D'] > 0.0)] # without sharks\n",
    "    \n",
    "    if len(shark_dat) > 0:\n",
    "        all_samples.append(shark_dat) # add all the data in to_sample\n",
    "        all_samples.append(no_sharks) # add non-shark data\n",
    "\n",
    "all_samples = pd.concat(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.to_csv('D:/Documents/SpringBoard/capstone-1/datasets/final_files/combined/filled/jackknifed_new.csv', index=False)\n",
    "    # save the file without index values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
